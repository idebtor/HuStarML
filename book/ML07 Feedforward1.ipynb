{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning For Everyone\n",
    "Lecture notes for HuStar Project by idebtor@gmail.com \n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 7 강: 순전파 1$^{Feedforward}$ \n",
    "\n",
    "## 학습 목표\n",
    "    - 삼층신경망의 예제로 순전파를 학습한다. \n",
    "\n",
    "## 학습 내용\n",
    "    - 삼층 신경망 예제\n",
    "    - 인공 신경망의 표기법\n",
    "    - 인공 신경망의 순전파 계산\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 삼층 신경망 예제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 신경망을 통한 신호를 처리할 때 행렬로 계산하지 않았습니다. 또한, 2개 이상의 층으로 이루어진 예제에 대해 다루지 않았습니다. 이 예제에서는 우리가 가운데 층의 출력을 마지막 세 번째 층의 입력으로 어떻게 다루어야 할지 살펴보아야 하기 때문에 흥미로울 것입니다.\n",
    "다음 그림은 3개의 층에 각각 3개의 노드를 포함하는 신경망 예제를 보여줍니다. 이 그림을 깔끔하게 보여주기 위해 가중치를 모두 다 표시하지는 않았습니다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_example.png?raw=true\" width=\"500\">\n",
    "<center>그림 1: 삼층 신경망의 구현 </center>\n",
    "\n",
    "여기에서도 흔히 사용되는 용어에 대해 소개할 것입니다. 첫 번째 층은 우리가 이미 알고 있듯이 입력층$^{input \\ layer}$입니다. 마지막 층은 출력층$^{output \\ layer}$이라는 것도 우리가 알고 있습니다. 가운데 있는 층은 은닉층$^{hidden \\ layer}$이라 합니다. 이는 이해하기 힘들고 어두운 느낌이 들지만 그렇지는 않습니다. 이 이름은 가운데 있는 층의 출력이 마지막 출력만큼 명백하지 않기 때문에 \"은닉(숨겨진)\"이라는 표현을 사용합니다. 믿기 힘들지만, 이 이름을 붙인 그 이상의 이유는 없습니다.\n",
    "\n",
    "앞에 그림에서 설명한 신경망 예제를 살펴봅시다. 세 개의 입력은 $0.9, 0.1, 0.8$입니다. 따라서 입력 행렬 $\\mathbf{X}$은 다음과 같고, 입력 행렬은 곧 입력층(0층) 노드의 출력이므로, 이는 $\\mathbf{A^{[0]}}$로 표기할 수 있습니다. 여기서 지수의 대괄호는 층의 일련번호를 표시합니다.  \n",
    "\n",
    "\\begin{align} \\mathbf{X} = \\mathbf{A^{[0]}} = \\begin{pmatrix} 0.9 \\cr 0.1 \\cr 0.8 \\end{pmatrix} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\mathbf{A^{[0]}}$는 입력 $\\mathbf{X}$와 같습니다. 입력 노드는 아무 역할도 하지 않고 넘기므로, 입력 $\\mathbf{X}$와 $\\mathbf{A^{[0]}}$는 똑 같습니다.    입력층에서는 입력을 모두 표현하는 것뿐이기 때문에 첫 번째 층은 우리 일이 끝났습니다.\n",
    "\n",
    "어렵지 않죠?\n",
    "\n",
    "다음은 중간에 있는 은닉층입니다. 여기서 우리는 중간 층의 각 노드로 들어가는 신호를 계산해야 합니다. 이 은닉층의 각 노드는 입력층의 모든 노드와 연결되어 있기 때문에, 각 입력 신호의 일부분을 얻게 된다는 것을 기억하세요. 우리는 이전과 같이 손으로 직접 많은 계산을 하기 원하지 않기 때문에 행렬을 사용하고자 합니다.\n",
    "\n",
    "그림에서 보듯이 은닉층으로 들어가는 입력이 가중치와 곱한 후 합산되기 때문에 이것을 이름하여 순입력(net input, 혹은 weighted Sum)이라고 부르며, 이는 연결된 노드 입력과 가중치를 곱하여 합산하여 계산할 수 있습니다 \n",
    "일반적으로 앞층에서 나오는 각 노드의 출력과 가중치를 곱하여 합산하는 식은 다음과 같이 일반화하여 표시할 수 있습니다. 아래 식에서 소문자 $l$은 층의 일련번호 입니다.\n",
    "\n",
    "\\begin{align}\n",
    "{Z^{[l]}} = W^{[l]} ∙ A^{[l-1]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 $\\mathbf{A^{[0]}}$은 이미 알고 있지만 가중치 행렬 $\\mathbf{W}$은 무엇일까요? 더 정확하게 말하자면, 첫 번째 가중치 행렬 즉 입력층과 은닉층 사이의 가중치는 $\\mathbf{W^{[1]}}$ 무엇일까요?\n",
    "\n",
    "그림에서는 이 예제에 대한 (임의로 만든) 일부 가중치를 보여주었지만 모두 보여주지는 않았습니다.  다음은 모든 가중치를 보여주며, 마찬가지로 임의로 만들어낸 것입니다. 이 예제에서 특별히 다른 것은 없습니다.\n",
    "\n",
    "\\begin{align} \\mathbf{W_{input\\_hidden}} = \\mathbf{W_{ih}} = \\mathbf{W^{[1]}} = \n",
    "\\begin{pmatrix} 0.9 & 0.3 & 0.4 \\cr 0.2 & 0.8 & 0.2 \\cr 0.1 & 0.5 & 0.6 \\end{pmatrix} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 신경망 그림과 같이 첫 번째 입력 노드와 가운데 은닉층 첫 번째 노드 사이의 가중치는 $w_{11} = 0.9$라는 것을 알 수 있습니다. 비슷하게, 그림에서 보여준 바와 같이 입력층의 두 번째 노드와 은닉층의 두 번째 노드 사이의 연결된 가중치는 $w_{22} = 0.8$이라는 것을 알 수 있습니다. 이 그림에서는 세 번째 입력 노드와 첫 번째 은닉층 노드 사이의 연결에 대해 보여주지 않지만, $w_{31} = 0.4$라고 만들어 냈습니다.\n",
    "\n",
    "하지만 잠깐만요. 이 $\\mathbf{W}$에 왜 __\"input_hidden\"__이란 이름을 덧붙였을까요? \n",
    "\n",
    "$\\mathbf{W}$은 입력층과 은닉층 사이의 가중치이기 때문입니다. 우리는 은닉층과 출력층 사이의 연결에 대한 가중치 행렬도 필요하며, 우리는 이것을 $\\mathbf{W_{input\\_hidden}}$ 라고 표시하거나 간단히 $\\mathbf{W_{ih}}$ 혹은 $\\mathbf{W^{[1]}}$이라고 표시할 것입니다. 다음에 우리가 필요한 것은 이미 정해 놓은 두 번째 행렬 $\\mathbf{W_{hidden\\_output}}$입니다. 마찬가지로, 여기에서는 세 번째 은닉 노드와 세 번째 출력 노드 사이의 연결된 가중치 $w_{33} = 0.9$ 등을 알 수 있습니다.\n",
    "\n",
    "\\begin{align} W_{hidden\\_output} = W_{ho} = W^{[2]} = \n",
    "\\begin{pmatrix} 0.3 & 0.7 & 0.5 \\cr 0.6 & 0.5 & 0.2 \\cr 0.8 & 0.1 & 0.9 \\end{pmatrix} \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "좋습니다. 가중치 행렬을 얻었습니다.\n",
    "\n",
    "이제 은닉층으로 들어가는 입력의 합이라는 것을 표현하기 위해 이를 설명하는 이름을 붙여야 하는데, 이를 $\\mathbf{X_{hidden}}$이라 부르고 혹은 간단히 순입력  $\\mathbf{Z^{[1]}}$로 표시할 수 있습니다. 아래 두 식은 동일한 내용입니다. \n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "X_{hidden} &= W_{input\\_hidden} • X \\\\\n",
    "Z^{[1]} &= W^{[1]} • A^{[0]}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "모든 행렬 곱셈을 하는 것은 전체 행렬을 사용하는 것이기 때문에 우리는 하지 않을 것입니다. 대신 컴퓨터가 힘든 숫자 계산을 하게 될 것입니다. 답은 아래와 같이 계산됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{aligned}\n",
    "X_{hidden} = W_{input\\_hidden} • X &= \n",
    "\\begin{pmatrix} 0.9 & 0.3 & 0.4 \\cr 0.2 & 0.8 & 0.2 \\cr 0.1 & 0.5 & 0.6 \\end{pmatrix} \n",
    "• \\begin{pmatrix} 0.9 \\cr 0.1 \\cr 0.8 \\end{pmatrix}  = \n",
    "\\begin{pmatrix} 0.16 \\cr 0.42 \\cr 0.62 \\end{pmatrix} \\\\\n",
    "Z^{[1]} = W^{[1]} • A^{[0]} &= \n",
    "\\begin{pmatrix} 0.9 & 0.3 & 0.4 \\cr 0.2 & 0.8 & 0.2 \\cr 0.1 & 0.5 & 0.6 \\end{pmatrix} \n",
    "• \\begin{pmatrix} 0.9 \\cr 0.1 \\cr 0.8 \\end{pmatrix}  = \n",
    "\\begin{pmatrix} 0.16 \\cr 0.42 \\cr 0.62 \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "\n",
    "Jupyter notebook에서 다음과 같이 실행해 볼 수 있다만, 먼저 이러한 계산을 손으로 한번 해볼 필요가 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16 0.42 0.62]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Wih = np.array([[0.9, 0.3, 0.4], [0.2, 0.8, 0.2], [0.1, 0.5, 0.6]])\n",
    "X = np.array([0.9, 0.1, 0.8])\n",
    "Xh = np.dot(Wih, A0)\n",
    "print(Xh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 가운데 은닉층으로 들어가는 조정된 입력을 합한 결과 즉 순입력($1.16, 0.42, 0.62$)을 알아냈습니다. 또한, 우리는 행렬을 사용하여 어려운 일을 해냈습니다. 이것은  자랑할 만한 성과입니다!\n",
    "\n",
    "가중치로 조정이 되어 두 번째 은닉층으로 들어온 것을 시각화 해봅시다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_example2.png?raw=true\" width=\"500\">\n",
    "<center>그림 2: 은닉층의 순입력 </center>\n",
    "\n",
    "아직까지는 아주 좋습니다. 하지만 우리가 더 해야 할 것이 있습니다. 여러분은 이 노드에서 시그모이드 활성화 함수를 적용하여 자연에서 찾을 수 있는 신호 응답과 유사하게 만든다는 것을 기억할 것입니다. 따라서 이것을 적용해봅시다.\n",
    "\n",
    "\\begin{aligned}\n",
    "O_{hidden} &= sigmoid(X_{hidden})\\\\\n",
    "A^{[1]} &= sigmoid(Z^{[1]})\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수는 $X_{hidden}$ 혹은 $Z^{[1]}$의 각 요소에 적용되어 가운데 은닉층의 출력을 나타내는 행렬을 만들게 됩니다.\n",
    "\\begin{aligned}\n",
    "O_{hidden} &= Z^{[1]} = sigmoid \\begin{pmatrix} 0.16 \\cr 0.42 \\cr 0.62 \\end{pmatrix} \n",
    "= \\begin{pmatrix} 0.76 \\cr 0.60 \\cr 0.65 \\end{pmatrix} \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 요소를 확인해봅시다. 시그모이드 함수는 $y=1/(1+e^{-x})$이기 때문에 $x = 1.16$일 때, $e^{-1.16}$은 $0.3135$입니다. 즉, $y = 1/(1+0.3135) = 0.761$입니다. \n",
    "손으로 직접 계산해보고, 또한 Jupyter notebook은 확인해보면 다음과 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76133271 0.60348325 0.65021855]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "Oh = sigmoid(Xh)\n",
    "print(Oh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, 모든 값이 0과 1 사이라는 것을 알 수 있을 것입니다. 그 이유는 시그모이드가 이 범위 밖의 값은 생산하지 않기 때문입니다. 로지스틱 함수 그래프를 다시 되돌아보고 이것을 시각적으로 확인해보세요.\n",
    "\n",
    "휴! 잠시 멈추고 우리가 지금까지 한 것을 돌아봅시다. 우리는 가운데 층을 지나는 신호를 계산하였습니다. 즉, 가운데 층의 출력값을 계산한 것입니다. 더 정확히 말하면, 가운데 층으로 들어가는 합해진 입력에 활성화 함수가 적용되었습니다. 이 새로운 정보로 그림을 수정해봅시다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_example3.png?raw=true\" width=\"500\">\n",
    "<center>그림 3: 활성화 함수를 적용한 은닉층의 출력 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것이 두 개의 층을 가진 신경망이었다면, 두 번째 층에서 나오는 출력을 이미 알고 있기 때문에 여기에서 멈추었을 것입니다. 우리가 여기에서 멈추지 않는 이유는 또 다른 세 번째 층이 있기 때문입니다.\n",
    "\n",
    "세 번째 층을 지나는 신호는 어떻게 처리할 수 있을까요? 두 번째 층과 같은 방법을 사용하며, 실제로 다른 것은 없습니다. 두 번째 층으로 들어오는 신호와 비슷하게 세 번째 층으로 들어오는 신호가 있습니다.\n",
    "\n",
    "또한, 우리가 자연에서 볼 수 있는 것과 같은 응답을 만들기 위해 활성화 함수를 적용합니다. 따라서 기억해야 할 것은, 몇 개의 층이 있던지 우리가 결합해야 할 들어오는 신호, 이 신호를 완화하는 연결된 가중치, 이 층에서 출력을 생산하기 위한 활성화 함수를 갖는 다른 층과 똑같이 각 층을 다룰 수 있다는 것입니다. 방법은 같기 때문에 우리가 3번째 또는 53번째, 아니면 103번째 층을 작업하고있더라도 신경 쓰지 않아도 됩니다.\n",
    "\n",
    "계속해서 이전에 우리가 했던 것과 같이 마지막 층에 들어가는 순입력 $Z = W ∙ A$을 계산해봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 층에 들어가는 입력은 우리가 방금 계산한 두 번째 층에서 나오는 출력 $O_{hidden}$입니다. 그리고 가중치 $W_{hidden\\_output}$는 두 번째 층과 세 번째 층 사이의 연결된 가중치입니다. 출력층의 입력되는 순입력의 계산 공식은 다음과 같습니다. \n",
    "\n",
    "\\begin{aligned}\n",
    "X_{output} &= W_{hidden\\_output} • O_{hidden} \\\\\n",
    "Z^{[2]} &= W^{[2]} • A^{[1]}\n",
    "\\end{aligned}\n",
    "\n",
    "이를 같은 방법으로 계산하면 마지막 출력층으로 들어가는 순입력은 다음과 같습니다.\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "X_{output} = W_{hidden\\_output} • O_{hidden} &= \n",
    "\\begin{pmatrix} 0.3 & 0.7 & 0.5 \\cr 0.6 & 0.5 & 0.2 \\cr 0.8 & 0.1 & 0.9 \\end{pmatrix} \n",
    "• \\begin{pmatrix} 0.76 \\cr 0.60 \\cr 0.65 \\end{pmatrix}  = \n",
    "\\begin{pmatrix} 0.98 \\cr 0.89 \\cr 1.25 \\end{pmatrix} \\\\\n",
    "Z^{[2]} = W^{[2]} • A^{[1]} &= \n",
    "\\begin{pmatrix} 0.3 & 0.7 & 0.5 \\cr 0.6 & 0.5 & 0.2 \\cr 0.8 & 0.1 & 0.9 \\end{pmatrix} \n",
    "• \\begin{pmatrix} 0.76 \\cr 0.60 \\cr 0.65 \\end{pmatrix}  = \n",
    "\\begin{pmatrix} 0.98 \\cr 0.89 \\cr 1.25 \\end{pmatrix} \n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook으로 계산을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97594736 0.88858496 1.25461119]\n"
     ]
    }
   ],
   "source": [
    "Who = np.array([[0.3, 0.7, 0.5], [0.6, 0.5, 0.2], [0.8, 0.1, 0.9]])\n",
    "Xo = np.dot(Who, Oh)\n",
    "print(Xo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 변경된 그림에서는 처음 입력에서 마지막 층으로 들어가는 결합된 입력까지의 신호 처리 과정을 보여줍니다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_example4.png?raw=true\" width=\"500\">\n",
    "<center>그림 4: 가중치 $W^{[2]}$와 은닉층의 출력$A^{[1]}$으로 구한 출력층의 순입력 $Z^{[2]}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 남은 것은 시그모이드 활성화 함수를 적용하는 것입니다.  이것은 참 쉽습니다.\n",
    "\n",
    "\\begin{aligned}\n",
    "O_{output} &= sigmoid(X_{output}) = sigmoid(\\begin{pmatrix} 0.98 \\cr 0.89 \\cr 1.25 \\end{pmatrix})\n",
    "= \\begin{pmatrix} 0.73 \\cr 0.71 \\cr 0.78 \\end{pmatrix} \\\\\n",
    "A^{[2]} &= sigmoid(Z^{[2]})\n",
    "\\end{aligned}\n",
    "\n",
    "끝났습니다! 신경망의 마지막 출력을 얻었습니다.  Jupyter  notebook으로 확인도 해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72630335 0.70859807 0.77809706]\n"
     ]
    }
   ],
   "source": [
    "Oo = sigmoid(Xo)\n",
    "print(Oo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_example5.png?raw=true\" width=\"500\">\n",
    "<center>그림 5: 삼층 인공 신경망의 출력 </center>\n",
    "\n",
    "세 개의 층을 가진 신경망 예제의 마지막 출력 $A^{[2]} = 0.73, 0.71, 0.78 $ 입니다.   신경망의 처음 입력에서 시작하여 층들을 지나 마지막 출력층을 나오기까지 성공적으로 신호를 따라갔습니다.\n",
    "\n",
    "이제는 무엇을 해야 할까요?\n",
    "\n",
    "다음 단계는 오차를 계산하기 위해 신경망의 출력과 학습 자료를 비교하는 것입니다. 우리는 이 오차를 이용하여 신경망이 출력을 개선할 수 있도록 수정해야 합니다.  이는 아마 이해하기 가장 어려울 것이기 때문에 이 개념을 쉽게 설명하도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__연습문제:__\n",
    "\n",
    "다음 신경망의 은닉층과 출력층의 입력값과 출력값 즉 $X_{hidden}, \\ O_{hidden}, \\   X_{output}, \\ O_{output}$을 손으로 직접 계산하십시오. 또한 그 결과값을 Jupyter  notebook으로 확인 하십시오. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_example6.png?raw=true\" width=\"500\">\n",
    "<center>그림 6: 삼층 인공 신경망의 순전파 계산하기 연습문제 </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리 \n",
    "    - 인공 신경망의 신호 전달\n",
    "    - 인공 신경망의 신호 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "_Rejoice always, pray continually, give thanks in all circumstances; for this is God’s will for you in Christ Jesus. (1 Thes 5:16-18)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
