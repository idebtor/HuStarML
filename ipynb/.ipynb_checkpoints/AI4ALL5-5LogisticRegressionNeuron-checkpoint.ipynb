{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fear of the LORD is the beginning of knowledge, but fools despise wisdom and discipline. Proverbs 1:7\n",
    "\n",
    "-------\n",
    "\n",
    "# Welcome to \"AI for All\"\n",
    "\n",
    "Lecture Notes by idebtor@gmail.com, Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5-5 로지스틱 회귀 뉴론 만들기\n",
    "\n",
    "    5.1 퍼셉트론\n",
    "    5.2 시그모이드 함수\n",
    "    5.3 로지스틱 손실함수\n",
    "    5.4 이진 분류를 위한 데이터셋 준비\n",
    "    5.5 로지스틱 회귀 뉴론 만들기\n",
    "    5.6 로지스틱 회귀 뉴론으로 단일층 신경망 만들기\n",
    "    5.7 사이킷런의 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당뇨병 환자 데이터셋을 이용하여 선형 회귀를 할 때는 데이터셋 전체를 사용하여 모델을 훈련했습니다. 그런데, 여기서 잠시 멈추어 생각을 해보아야 합니다. 훈련된 모델이 실전에서 얼마나 좋은 성능을 내는지 어떻게 알 수 있을까요? 여러분이 만든 모델의 성능을 평가하지 않고, 실전에 투입하면 잘못된 결과를 초래할 수도 있으니 위험합니다. 또 훈련 데이터셋으로 학습된 모델을 다시 훈련 데이터셋로 평가하면 어떨까요? 만약 모델이 훈련 데이터셋를 몽땅 외워버렸다면(실제 이런 것이 가능합니다) 평가를 해도 의미가 없을 것입니다. 그러면 어떻게 해야 모델의 성능을 제대로 평가할 수 있을까요? 모델을 만들기 전에 성능 평가에 대해 알아보도록 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모델 성능 평가를 위한 훈련 셋과 테스트 셋\n",
    "\n",
    "훈련된 모델의 실전 성능을 일반화 성능(generalization performance)이라고 부릅니다. 만약, 훈련한 데이터셋으로 성능을 평가하면 당연히 좋은 성능이 나올 것입니다. 이런 성능 평가를 '과도하게 낙관적으로 일반화 선응을 추정한다'고 말합니다. \n",
    "\n",
    "그러면, 올바르게 모델의 성능 축정하려면 어떻게 해야 할까요? 훈련 데이터 셋을 두 부분으로 나누어 하는 훈련에, 또 다른 하나는 테스트에 사용하면 됩니다. 이때 각각의 데이터셋으로 구분하여 하나는 훈련셋(training-set) 다른 하나는 테스트셋(test set)이라고 부릅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터셋을 훈련셋과 테스트셋으로 나눌 때는 다음 2가지 규칙을 지켜야 합니다. \n",
    "\n",
    "> 훈련 데이터셋을 훈련셋과 테스트셋으로 나누는 규칙\n",
    "> - 훈련 데이터셋을 나눌 때는 테스트 셋보다 훈련셋이 더 많아야 한다. \n",
    "> - 훈련 데이터셋을 나누기 전에 양성, 음성 클래스가 훈련셋과 테스트셋의 어느 한쪽에 몰리지 않도록 골고루 섞어야 한다. \n",
    "\n",
    "위 과정은 사이킷런에 준비되어 있는 도구를 사용하면 편리하게 진행할 수 있습니다. 그러면, 지금부터 훈련 데이터셋을 둘로 분리하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 훈련셋과 테스트셋으로 나누기\n",
    "\n",
    "훈련 데이터셋을 훈련셋과 테스트셋으로 나눌 때는 양성, 음성 클래스가 두 셋들에 고르게 분포하도록 만들어야 합니다. 예를 들면, cacner 데이터셋를 보면 음성 클래스(정상 종양)와 양성클래스(악성 종양)의 샘플 갯수각 각각 213, 357 개입니다. 이 클래스 비율이 두 셋에도 그대로 유지되어야 올바르게 학습할 수 있고, 또한 성능도 제대로 평가할 것입니다. \n",
    "\n",
    "지금부터 음성 클래스와 양성 클래스의 비율을 일정하게 유지하면서 훈련 데이터셋을 둘 로 나누어 보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. train_test_split() 함수로 훈련 데이터셋 나누기\n",
    "\n",
    "먼저 sklearn.model_selection모듈에서 train_test_split()함수를 import합니다.  train_test_split()함수는 기본적으로 입력된 훈련 데이터 셋을 훈련셋 75%, 테스트셋 25%의 비율로 나누어 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음 train_test_split() 함수에 입력 데이터셋 x, 타깃 데이터셋 y와 그 밖의 설정을 매개변수로 지정하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-398ffaa9be90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[1;32m-> 2155\u001b[1;33m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[1;32m-> 2155\u001b[1;33m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_list_indexing\u001b[1;34m(X, key, key_dtype)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;31m# key is a integer array-like of key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;31m# key is a integer array-like of key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매개 변수 설정에 대한 내용은 다음과 같습니다. \n",
    "\n",
    "- stratify=y : stratify는 훈련 데이터셋을 나눌 때 클래스 비율을 동일하게 만듭니다. train_test_split()함수는 기본적으로 데이터를 나누기 전에 섞지만 일부 클래스 비율이 불균형한 경우에는 stratify를 y로 지정해야 합니다.\n",
    "\n",
    "- test_size = 0.2 : train_test_split() 함수는 기본적으로 훈련 데이터셋을 3:1 비율로 나눕니다. 하지만 필요한 경우 이 비율을 조절하고 싶을 때도 있습니다. 그러한 경우 test_size 매개변수에 테스트셋의 비율을 전달하면 비율을 조절할 수 있습니다. 여기서 0.2는 테스트셋의 크기를 20%로 하겠다는 것입니다. \n",
    "\n",
    "- random_state=42 : train_test_split() 함수는 무작위로 데이터셋을 섞은 다음 나눕니다. 여기서는 섞은 다음 나눈 결과가 항상 일정하도록 random_state 매개변수에 난수 초깃값을 42로 지정한 것입니다. (이렇게 하는 것이 디버깅할 때 유익합니다.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 결과 확인하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면, 훈련 데이터셋이 훈련셋과 테스트셋으로 잘 나누어졌는디, 그 비율을 확인해 보겠습니다. shape속성을 이용해 확인해 보면, 각각의 셋이 4:1의 비율(test_size = 0.2)로 잘 나누어졌습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. unique()함수로 훈련셋의 타깃 확인하기\n",
    "\n",
    "넘파이의 unique()함수로 훈련셋의 타깃 안에 있는 클래스의 개수를 확인해보니 전체 훈련 데이터셋의 클래스 비율과 거의 비슷한 구성입니다. (양성 클래스가 음성 클래스보다 1.7배정도 많습니다) 클래스의 비율이 그대로 유지되고 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([170, 285], dtype=int64))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 로지스틱 회귀 구현하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련셋이 준비되었으니, 본격적으로 로지스틱 회귀를 구현하도록 합니다. 로지스틱 회귀는 순방향으로 데이터가 흘러가는 과정과 가중치를 조정하기 위해 역방향으로 데이터가 흘러가는 과정을 구현해야 합니다. 순방향 계산부터 역방향 계산까지 순서대로 구현하도록 합니다. 여기서 만들 LogisticNeuron 클래스의 매소드는 앞에서 구현한 Neuron 클래스의 `__init__(), forpass(), backprop()` 메소드와 거의 동일합니다. 다음을 입력하십시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticNeuron:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def forpass(self, x):\n",
    "        z = np.sum(x * self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def backprop(self, x, err):\n",
    "        w_grad = x * err        # 가중치에 대한 gradient를 계산함\n",
    "        b_grad = 1 * err        # 편향에 대한 gradient를 계산함\n",
    "        return w_grad, b_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 앞에서 코딩한 Neuron클래스와 별로 다르지 않지만, 몇가지 세부 설정이 바뀌었습니다. 다음은 각 메소가 Neuron클래스와 비교하여 어떻게 달라졌는지 설명합니다. \n",
    "\n",
    "- `__init__()` __메소드는 가중치와 편향을 미리 초기화하지 않습니다.__ 여기서 `__int__()`메소드를 보면, 입력 데이터의 특성이 많아 가중치를 미리 초기화 하지 않았습니다. 가중치는 나중에 입력 데이터를 보고, 특성 개수에 맞게 결정합니다. \n",
    "\n",
    "- `forpass()` __ 메소드를 보면 가중치의 입력 특성의 곱을 모두 더하기 위해 np.sum()을 사용했습니다. x * self.w 에서 x와 w는 1차원 넘파이 배열인데, 넘파이 배열에서 사칙 연산을 적용하면 자동으로 배열의 요소끼리 계산합니다. 예를 들면, 넘파이 배열 `[1, 2, 3]` 과 `[4, 5, 6]`을 더하면, `[4, 6, 8]`이라는 넘파이 배열이 나오고 곱하면 `[3, 8, 15]`배열이 나옵니다. 또 넘파이 배열을 sum()함수의 인자로 전달하면, 각 요소를 모두 더한 값을 반환합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 8]\n",
      "[ 3  8 15]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([3, 4, 5])\n",
    "print(a + b)\n",
    "print(a * b)\n",
    "print(np.sum(a * b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 뉴론이 구현되었습니다. 이제 훈련하고 예측하는 일만 남았습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련하는 메소드 구현하기\n",
    "\n",
    "훈련을 수행하는 fit()메소드를 구현하기로 합니다. \n",
    "\n",
    "#### 1. fit() 메소드 구현하기 \n",
    "\n",
    "fit() 메소드의 가장 기본 구조는 Neuron클래스와 같습니다. 다만 활성화 함수가 추가된 점이 다릅니다. 로지스틱 회귀에는 이진 분류를 위한 활성화 함수인 시그모이드 함수가 필요했던 것을 기억하십시오. activation()메소드는 바로 다음에 구현합니다. 역방향 계산에는 로지스틱 손실 함수의 도함수를 적용합니다. 앞에서 초기화하디 않은 가중치는 np.ones() 함수를 이용하여 간단히 1로 초기화하고 편향은 0으로 초기화 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, x, y, epochs = 100):\n",
    "    self.w = np.ones(x.shape[1])        # 가중치 초기화 \n",
    "    self.b = 0                          # 편향 초기화\n",
    "    for i in range(epochs):             # epochs만큰 반복하기\n",
    "        for x_i, y_i in zip(x, y):      # 모든 샘플에 대해 반복하기\n",
    "            z = self.forpass(x_i)       # 순방향 계산\n",
    "            a = self.activation(z)      # 활성화 함수 적용\n",
    "            err = -(y_i - a)            # 오차 계산\n",
    "            w_grad, b_grad = self.backprop(x_i, err)  # 역방향 계산\n",
    "            self.w -= w_grad            # 가중치 조정\n",
    "            self.b -= b_grad            # 편향 조정\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. activation() 메소드 구현하기 \n",
    "\n",
    "activation() 메소드에는 시그모이드 함수가 사용되어야 합니다. 시그모이드 함수는 자연 상수의 지수 함수를 계산하는 넘파이의 np.exp()함수를 사용하면 간단히 만들 수 있습니다. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(self, z):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 예측하는 메소 구현하기\n",
    "\n",
    "선형 회귀에서 새로운 샘플에 대한 예측값을 계산할 때 forpass()메소드를 사용했습니다. 여러 개의 샘플을 한꺼번에 예측하려면 forpass()메소드를 여러 ㅂㄴ 호출하게 되는데 이 작업은 번거롭습니다. 분류에서는 활성화 함수와 임계함수도 적용해야 하므로 새로운 샘플에 대한 예측값을 계산해주는 메소드인 predict() 메소드를 만들어 보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict() 메소드 구현하기\n",
    "\n",
    "predict()메소드의 매개변수 값으로 입력값 x가 2차원 배열로 전달된다고 가정하고 구현합니다. 예측값은 입력값을 선형 함수, 활성화 함수, 임계 함수 순서로 통과시키면 구할 수 있습니다. 앞에서 forpass()와 activation()메소드를 이미 구현했으니 predict()메소드는 다음과 같이 간단하게 구현할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    z = None       # 선형 함수 적용 for x\n",
    "    a = None       # 활성화 함수 적용 for z \n",
    "    return None   # 계단 함수 적용 a > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 z의 계산으로 파이썬의 리스트 내포(list comprehension)문법을 사용했습니다. 리스트 내포란 대괄호([])안에 for 문을 삽입하여 새 리스트를 만드는 간결한 문법입니다. x의 행을 하나씩 꺼내어 forpass() 메소드에 적용하고 그 결과를 이용하여 새 리스트 z로 만드는 것입니다. z는 곧바로 넘파이 배열로 바꾸어 activation() 메소드에 전달합니다. 자, 이제 로지스틱 회귀가 구현되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 완성된 LogisticNeuron 클래스 한 눈에 보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticNeuron:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def forpass(self, x):\n",
    "        z = np.sum(x * self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def backprop(self, x, err):\n",
    "        w_grad = x * err        # 가중치에 대한 gradient를 계산함\n",
    "        b_grad = 1 * err        # 편향에 대한 gradient를 계산함\n",
    "        return w_grad, b_grad \n",
    "\n",
    "    def activation(self, z):\n",
    "        z = np.clip(z, -500, 500)       # 오버플로우 방지를 위해\n",
    "        return 1 / (1 + np.exp(-z)) \n",
    "\n",
    "    def fit(self, x, y, epochs = 100):\n",
    "        self.w = np.ones(x.shape[1])        # 가중치 초기화 \n",
    "        self.b = 0                          # 편향 초기화\n",
    "        for i in range(epochs):             # epochs만큰 반복하기\n",
    "            for x_i, y_i in zip(x, y):      # 모든 샘플에 대해 반복하기\n",
    "                z = self.forpass(x_i)       # 순방향 계산\n",
    "                a = self.activation(z)      # 활성화 함수 적용\n",
    "                err = -(y_i - a)            # 오차 계산\n",
    "                w_grad, b_grad = self.backprop(x_i, err)  # 역방향 계산\n",
    "                self.w -= w_grad            # 가중치 조정\n",
    "                self.b -= b_grad            # 편향 조정\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = [self.forpass(x_i) for x_i in x]   # 선형 함수 적용\n",
    "        a = self.activation(np.array(z))       # 활성화 함수 적용\n",
    "        return a > 0.5                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 로지스틱 회귀 모델 훈련시키기\n",
    "\n",
    "이제 준비한 데이터셋을 사용하여 로지스틱 회귀 모델을 훈련해보고 정확도를 측정해보겠습니다. \n",
    "\n",
    "#### 1. 모델 훈련 하기\n",
    "\n",
    "모델을 훈련하는 방법은 Neuron과 동일합니다. LogisticNeuron 클래스의 객체를 만든 다음, 훈련셋과 함께 fit()메소드를 호출하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = LogisticNeuron()\n",
    "neuron.fit(None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 테스트셋 사용해 모델의 정확도 평가하기\n",
    "\n",
    "위 코드를 통해 훈련이 끝난 모델에 테스트셋을 사용해 예측값을 넣고 예측한 값이 맞는지 비교합니다. 다음 코드의 결과는 무엇입니까? 그 결과를 활용하여 정확도를 평가할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.predict(x_test) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.None(neuron.predict(x_test) == y_test) / None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹은 더 간단히..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8245614035087719"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.None(neuron.predict(x_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict() 메소드의 반환값은 True나 False로 채원(m,)크기의 배열이고, y_test()는 0 또는 1로 채워진 (m,)크기의 배열이므로 바로 비교할 수 있습니다. np.mean()함수는 매개벼수 값으로 전달한 비교문 결과(넘파이 배열)의 평균을 계산합니다. 즉, 계산 결과 0.82... 는 올바르게 예측한 샘플의 비율이 됩니다. 이를 정확도(accuracy)라고 합니다. \n",
    "\n",
    "로지스틱 회귀가 제대로 구현된 것 같습니다. 아주 훌륭하진 않지만 82%의 정확도가 나왔네요. 사실 우리가 구현한 LogisticNeuron클래스의 성능은 좋은 편이 아닙니다. 실전에서는 사이킷런과 같은 안정적인 패키지 사용을 권합니다. 지금은 학습을 위해 구현한 것이므로 성능에 크게 신경쓰지 않아도 됩니다. 이제 마지막으로 하나의 층(layer)을 가진 신경망을 구현해보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "\n",
    "1. 케라스 창시자에게 배우는 딥러닝, 프랑소와 숄레, 길벗\n",
    "1. 핸즈온 머신러닝, 오렐리앙 제롱, 한빛미디어\n",
    "1. 딥러닝 입문, 박해선, 이지스 퍼블리싱\n",
    "1. 파이썬으로 배우는 기계학습, 김영섭, K-MOOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
