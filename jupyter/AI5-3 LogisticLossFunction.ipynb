{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나님이 자기 형상 곧 하나님의 형상대로 사람을 창조하시되 남자와 여자를 창조하시고 하나님이 그들에게 복을 주시며 하나님이 그들에게 이르시되 생육하고 번성하여 땅에 충만하라, 땅을 정복하라, 바다의 물고기와 하늘의 새와 땅에 움직이는 모든 생물을 다스리라 하시니라 (창1:27-28)\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/joyai/LectureNotes_ML.png?raw=true\" width=1000></center>\n",
    "\n",
    "__NOTE:__ The following materials have been compiled and adapted from the numerous sources including my own. Please help me to keep this tutorial up-to-date by reporting any issues or questions. Send any comments or criticisms to `idebtor@gmail.com` Your assistances and comments will be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"font-size:30px\"> Chapter 5-3 로지스틱 손실함수(Logistic loss fucntion) </b>\n",
    "\n",
    "    5.1 퍼셉트론  \n",
    "    5.2 로지스틱 회귀와 시그모이드 함수    \n",
    "    5.3 로지스틱 손실함수    \n",
    "    5.4 이진 분류를 위한 데이터셋 준비    \n",
    "    5.5 로지스틱 회귀 뉴론 만들기    \n",
    "    5.6 로지스틱 회귀 뉴론으로 단일층 신경망 만들기    \n",
    "    5.7 사이킷런의 로지스틱 회귀    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"font-size:16px\"> 로지스틱 회귀 중간 정리하기 </b>\n",
    "\n",
    "로지스틱 회귀에 필요한 설명을 모두 마쳤습니다. 이제는 다음 그림을 보며 로지스틱 회귀를 정리합니다. \n",
    "\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/joyai/ai4all-neuron8.png?raw=true\" width=600></center>\n",
    "<center> 그림 1: 로지스틱 회귀의 구조 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__로지스틱 회귀는 이진 분류가 목표이므로 $-\\infty$부터 $\\infty$의 범위를 가지는 $z$의 값을 조절할 방법이 필요했습니다.__ 그래서 시그모이드 함수를 활성화 함수로 사용한 것입니다. 이는 시그모이드 함수를 통과하면 $z$를 확률처럼 해석할 수 있기 때문입니다. 그리고, 시그모이드 함수의 확률인 $a$를 0과 1로 구분하기 위하여 마지막에 임계 함수를 사용했습니다. \n",
    "\n",
    "그 결과 입력 데이터 $x$는 0과 1의 값으로 나누어졌습니다. 즉, 이진 분류가 되었습니다. 드디어 로지스틱 회귀가 '이진 분류'를 하기 위한 알고리즘인 진짜 이유를 알았습니다. \n",
    "\n",
    "이제 무엇이 남았습니까?   \n",
    "\n",
    "아직 우리는 가중치와 편향을 적절하게 조정할 수 있는 방법을 배우지 않았습니다. 그렇다면 로지스틱 회귀에는 어떤 손실 함수를 사용해야 할까요? 선형 회귀에서 손실 함수로 제곱 오차를 사용했듯이 분류 문제에서도 __제곱 오차__ 를 사용할 수 있을까요? \n",
    "\n",
    "이제 로지스틱 회귀를 위한 손실 함수인 __로지스틱 손실 함수__ 에 대해 알아 보도록 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 손실 함수\n",
    "\n",
    "선형 회귀는 정답과 예측 값의 오차 제곱이 최소가 되는 __가중치와 편향__ 을 찾는 것이 목표입니다.  \n",
    "그렇다면, 로지스틱 회귀와 같은 분류의 목표는 무엇일까요? \n",
    "\n",
    "올바르게 분류된 샘플 데이터의 비율을 높이는 것이 분류의 목표입니다. 예를 들면, 사과, 배, 감을 분류하는 문제에서 사과, 배, 감으로 분류한 과일 중에서 제대로 분류한 비율을 높이는 것이 분류의 목표입니다. 하지만 안타깝게도 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니기 때문에 경사 하강법의 손실 함수로 사용할 수 없습니다.  대신 비슷한 목표를 달성할 수 있는 다른 함수를 사용해야 하는데, 그 함수가 바로 __로지스틱 손실 함수__ 입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 이제 이 로지스틱 함수를 알아봅시다. \n",
    "__로지스틱 손실 함수__ 는 다중 분류를 위한 손실 함수인 __크로스 엔트로피(cross entropy)손실 함수__ 를 이진 분류 버전으로 만든 것입니다. 크로스 엔트로피 손실 함수는 다중 분류를 다룰 때 공부합니다. 실무에서는 이중분류와 다중 분류를 분리하지 않고 모두 크로스 엔트로피 손실 함수라고 부르는 경우도 많습니다. 하지만 공부하는 입장에서는 둘을 구분하여 사용하기로 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__로지스틱 손실 함수__ 는 다음과 같습니다. $a$는 활성화 함수가 출력한 값이고 $y$는 타깃(레이블)입니다. \n",
    "\n",
    "\\begin{align}\n",
    "L = - (y log(a) + (1-y) log(1-a))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 어떻게 이해하면 좋을까요?\n",
    "\n",
    "이진 분류는 그렇다(1), 아니다(0)라는 식으로 2개의 정답만 있습니다. 즉, 타깃의 값은 1 또는 0입니다. 따라서 위 식은 $y$가 1이거나 0인 경우로 정리됩니다. \n",
    "\n",
    "|                               |  $L$          |\n",
    "|:-----------------------------:|:-------------:|\n",
    "| $y$가 1인 경우(양성 클래스)   |    $-log(a)$   |  \n",
    "| $y$가 0인 경우(음성 클래스)   |    $-log(1-a)$ |   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데, 앞 두 식의 값을 최소로 만들다 보면 $a$의 값이 우리가 원하는 목표치가 된다는 것을 알 수 있습니다. \n",
    "\n",
    "예를 들어, 양성 클래스인 경우 로지스틱 손실 함수의 값을 최소로 만들려면, $a$는 1에 자연스럽게 가까워집니다. 반대로 음성 클래스인 경우, 로지스틱 손실 함수의 값을 최소 만들면 $a$가 0에 가까워집니다.  이 값을 계단 함수에 통과시키면 올바르게 분류 작업이 수행됩니다. \n",
    "\n",
    "즉, 로지스틱 손실 함수를 최소화하면 $a$의 값이 우리가 가장 이상적으로 생각하는 값이 됩니다. 이제 로지스틱 손실 함수의 최솟값을 만드는 가중치와 편향을 찾기 위해 미분만 하면 될 것 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/joyai/ai4all-LogisticLoss.png?raw=true\" width=400></center>\n",
    "<center> 그림 1: 로지스틱 손실 함수 </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-6a924d9ed098>:5: RuntimeWarning: invalid value encountered in log\n",
      "  plt.plot(a, -np.log(a), label='-log(a), y = 1')\n",
      "<ipython-input-6-6a924d9ed098>:6: RuntimeWarning: invalid value encountered in log\n",
      "  plt.plot(a, -np.log(1 - a),label='-log(1 - a), y = 0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "a = np.linspace(-0.2, 1.2, 23)\n",
    "plt.plot(a, -np.log(a), label='-log(a), y = 1')\n",
    "plt.plot(a, -np.log(1 - a),label='-log(1 - a), y = 0')\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.xlabel('a, (probability)', fontsize = 14)\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.yticks([0.0, 1.0])\n",
    "plt.xticks([0.0, 1.0])\n",
    "plt.title('Logistic Loss Function', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Closer Look\n",
    "\n",
    "__로지스틱 손실함수__ 는 다음과 같이 조금 더 자세히 표현하기도 합니다. \n",
    "\n",
    "\\begin{align}\n",
    "L = -\\frac{1}{m}\\sum^{m}_{i=1}[y^{(i)} log(a^{(i)}) + (1-y^{(i)}) log(1-a^{(i)})]\n",
    "\\end{align}\n",
    "\n",
    "- `m`은 전체 데이터의 개수입니다.\n",
    "- $y^{(i)}$는 `i`번때 데이터의 class입니다. \n",
    "- $a^{(i)}$는 `i`번째 데이터의 log-odds값에 sigmoid를 취한 값입니다. 즉 `i`번째 데이터가 postive class에 속할 확률을 나타낸 값입니다. (0 <= $a^{(i)}$ <= 1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 위와 같은 공식이 유도되는 걸까요? 논리적으로 생각해 봅시다. 만약 i 번째 데이터의 class가 y=1 이라면 해당 데이터에 대한 loss는 다음과 같습니다.\n",
    "\n",
    "\\begin{align}\n",
    "loss_{i(y=1)} = -log(a^{(i)})\n",
    "\\end{align}\n",
    "\n",
    "$loss$를 최소화 시키려면 $a^{(i)}$ 값이 커야 합니다. 즉, 예측된 확률 값이 원래 class인 `1` 에 가까울수록 $loss$는 줄어들게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 반대로 `i` 번째 데이터의 class가 `y=0` 인 경우를 생각해봅시다.\n",
    "\n",
    "\\begin{align}\n",
    "loss_{i(y=0)} = -log(1 - a^{(i)})\n",
    "\\end{align}\n",
    "\n",
    "$loss$를 최소화 시키려면  $a^{(i)}$ 값이 작아야 합니다. 즉, 예측된 확률 값이 원래 class이 `0`에 가까울수록 $loss$는 줄어들게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그래프를 보면 올바르게 예측할수록 loss가 줄어드는 것을 볼 수 있습니다. 반대로 잘못 예측하게 되면 loss가 크게 증가하는데, 이는 모델이 잘못 예측할 때 패널티를 강하게 줌으로써 올바른 예측을 할 수 있도록 유도할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 손실 함수 미분의 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치와 편향에 대한 로지스틱 손실 함수의 미분 결과는 다음과 같습니다. 이 식은 가중치와 편향을 조정하는데 사용할 것입니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{}}{\\partial{w_i}} L &= -(y - a) x_i  \\\\\n",
    "\\frac{\\partial{}}{\\partial{b}} L  &= -(y - a) 1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 미분한 결과를 자세히 보면 $\\hat{y}$이 $a$로 바뀌었을 뿐 제곱 오차를 미분한 결과와 동일합니다. 놀랍죠? 아래 표에서 왼쪽이 제곱 오차의 미분이고 오른쪽이 로지스틱 손실 함수의 미분입니다. \n",
    "\n",
    "\n",
    "\n",
    "|                     |  제곱 오차 손실 함수의 미분     |  로지스틱 손실 함수의 미분 | \n",
    "|:-------------------:|:----------------------------------------:|:--------------------------:|\n",
    "| 가중치에 대한 미분  | $\\frac{\\partial{SE}}{\\partial{w}}=-(y - \\hat{y})x$ | $\\frac{\\partial{}}{\\partial{w_i}} L = -(y - a) x_i$\n",
    "| 편향에 대한 미분    | $\\frac{\\partial{SE}}{\\partial{b}}=-(y - \\hat{y})1 $| $\\frac{\\partial{}}{\\partial{b}} L  = -(y - a) 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 결과를 비추어보아 앞에서 우리가 구현한 Neuron 클래스와 크게 다르지 않을 것이라는 생각이 들 것입니다. 그러면, 이제 어떻게 이런 식이 유도되는지 살펴보도록 하겠습니다. 다음 과정이 좀 어렵다라도 기계학습과 딥러닝을 더 깊이 있게 이해하려면, 한번은 공부하고 넘어가는 것이 좋을 것입니다. \n",
    "\n",
    "로지스틱 손실 함수의 미분을 통해 로지스틱 손실 함수의 값을 최소로 하는 가중치와 편향을 찾아야 한다는 점을 꼭 기억해야 합니다. 그리고, 그 결과는 놀랍게도 제곱 오차 손실 함수의 미분과 거의 같다라는 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 로지스틱 손실 함수 미분하기\n",
    "\n",
    "### 로지스틱 손실 함수와 연쇄 법칙\n",
    "\n",
    "미분에서는 합성 함수의 도함수(미분한 함수)를 구하기 위한 방법인 연쇄법칙(Chain rule)이 있습니다. 예를 들어, 다음과 같은 함수는 \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "y = f(u), u = g(x)\n",
    "\\end{align}\n",
    "\n",
    "아래와 같이 정리할 수 있는데, \n",
    "\\begin{align}\n",
    "y = f(g(x))\n",
    "\\end{align}\n",
    "\n",
    "이때 $y$를 $x$에 대해 미분하는 방법은 $y$를 $u$에 대해 미분한 값과 $u$를 $x$에 대해 미분한 값을 곱하는 것입니다. 이것이 미분의 __연쇄 법칙__(Chain Rule)입니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{y}}{\\partial{u}} \\frac{\\partial{u}}{\\partial{x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 연쇄법칙을 설명한 이유는 로지스틱 손실 함수($L$)를 가중치($w$)나 편향($b$)에 대하여 바로 미분하면 너무 복잡하기 때문입니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{w_i}} = ? \\\\\n",
    "\\frac{\\partial{L}}{\\partial{b}} = ? \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데, 다음 그림을 살펴보면, 연쇄 법칙을 이용하면 위의 곤란한 문제를 해결할 수 있다는 힌트를 얻을 수 있습니다. \n",
    "\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/joyai/ai4all-neuron9.png?raw=true\" width=600></center>\n",
    "<center> 그림 2: 로지스틱 손실 함수의 미분과 연쇄 법칙 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림을 보니 로지스틱 손실 함수($L$)를 활성화 함수의 출력값($a$)에 대하여 미분하고, 활성화 함수 출력값($a$)를 선형 함수의 출력값($z$)에 대하여 미분하고, 선형 함수의 출력값($z$)은 가중치($w$) 또는 편향($b$)에 대하여 미분한 다음 서로 곱하면 결국 우리가 원하는 로지스틱 손실 함수를 가중에 대하여 미분한 결과를 얻을 수 있습니다.  그리고, 이 과정은 그림의 오른쪽부터 왼쪽까지 역방향으로 진행된다는 것도 알 수 있습니다. \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{w_i}} = \\frac{\\partial{L}}{\\partial{a}} \\frac{\\partial{a}}{\\partial{z}}  \\frac{\\partial{z}}{\\partial{w_i}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: 로지스틱 손실 함수를 $a$에 대하여 미분하기\n",
    "\n",
    "그러면, 이제 각각의 도함수를 구하기만 하면 됩니다. 먼저 로지스틱 손실 함수를 $a$에 대하여 미분하겠습니다. 이때 $y$는 $a$의 함수가 아니므로 미분 기호 밖으로 뺄 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{a}} \n",
    "&= \\frac{\\partial{}}{\\partial{a}} (-(ylog(a)+(1-y)log(1-a))) \\\\\n",
    "&= -(y\\frac{\\partial{}}{\\partial{a}} log(a)+(1-y) \\frac{\\partial{}}{\\partial{a}} log(1-a))\n",
    "\\end{align}\n",
    "\n",
    "$log(a)$을 $a$에 대하여 미분하면 $\\frac{1}{a}$이므로 위 식은 다음과 같이 간단하게 정리됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{a}} = -(y\\frac{1}{a} - (1 - y) \\frac{1}{1-a})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  $a$를 $z$에 대하여 미분하기\n",
    "\n",
    "이제 $\\frac{\\partial{a}}{\\partial{z}}$를 계산해 보겠습니다. \n",
    "\n",
    "여기에서 $a$는 시그모이드 함수이므로 $a$를 $z$에 대한 식으로 표현할 수 있습니다. $e^{-z}$를 $z$에 대하여 미분하면 $-e^{-z}$가 되므로 다음과 같이 미분할 수 있습니다. \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{a}}{\\partial{z}} \n",
    "&= \\frac{\\partial{}}{\\partial{z}}(\\frac{1}{1 + e^{-z}}) \\\\\n",
    "&= \\frac{\\partial{}}{\\partial{z}}(1 + e^{-z})^{-1} \\\\\n",
    "&= -(1 + e^{-z})^{-2} \\frac{\\partial{}}{\\partial{z}} (e^{-z}) \\\\\n",
    "&= -(1 + e^{-z})^{-2} (-e^{-z}) \\\\\n",
    "&= \\frac{e^{-z}}{ (1 + e^{-z})^2 } \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 얻은 식을 두 덩어리의 분수식으로 나눈 다음 공통 식을 묶어 정리하면 다음과 같이 됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{a}}{\\partial{z}} \n",
    "&= \\frac{e^{-z}}{ (1 + e^{-z})^2 } \\\\\n",
    "&= \\frac{1}{1 + e^{-z}} \\frac{e^{-z}}{ 1 + e^{-z} } \\\\\n",
    "&= \\frac{1}{1 + e^{-z}} (1 - \\frac{1}{ 1 + e^{-z} }) \\\\\n",
    "&= a ( 1 - a )\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결국 $a$를 $z$에 대해 미분하면 다음과 같은 식이 됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{a}}{\\partial{z}} = a ( 1 - a )\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  $z$를 $w$에 대하여 미분하기\n",
    "\n",
    "마지막으로 $\\frac{\\partial{z}}{\\partial{w_i}}$를 계산합니다. $z$는 선형 함수 $w_ix_i + b$이므로 $w_i$에 대해 미분하면 다른 항은 모두 사라지고 $x_i$만 남아   $\\frac{\\partial{z}}{\\partial{w_i}} = x_i$가 됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{z}}{\\partial{w_i}} = x_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: 각 단계에서 구한 도함수 곱하기 \n",
    "\n",
    "로지스틱 손실 함수를 $w$에 대하여 미분하기 위하여 연쇄법칙을 사용하여, 세 가지로 나무어 계산하였습니다. 이제 각 단계에서 구한 도함수를 곱하기만 하면 됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{w_i}} \n",
    "&= \\frac{\\partial{L}}{\\partial{a}} \\frac{\\partial{a}}{\\partial{z}}  \\frac{\\partial{z}}{\\partial{w_i}} \\\\\n",
    "&= -(y\\frac{1}{a} - (1 - y) \\frac{1}{1-a}) a ( 1 - a )  x_i \\\\\n",
    "&= -(y (1-a) - (1-y)a)  x_i  \\\\\n",
    "&= -y(y-ya-a+ya)x_i  \\\\\n",
    "&= -(y-a)x_i\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보니, 놀랍게도, 로지스틱 손실 함수를 $w_i$에 대해 미분한 결과는 제곱 오차를 미분한 결과와 일치합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 손실 함수의 미분 정리하고 역전파 이해하기\n",
    "\n",
    "다음은 지금까지 살펴본 미분 과정을 그림으로 나타낸 것입니다. \n",
    "\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/joyai/ai4all-neuron10.png?raw=true\" width=600></center>\n",
    "<center> 그림 3: 로지스틱 손실 함수의 미분 과정과 결과 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오른쪽부터 살펴보면 로지스틱 손실 함수 $L$은 $a$에 대하여 미분하고, $a$는 $z$에 대하여 미분하고, $z$는 $w$에 대하여 미분합니다. 그리고 각 도함수의 곱을 가중치를 조정하는데 사용합니다. 이렇게 로지스틱 손실 함수에 대한 미분이 연쇄법칙에 의해 진행되는 구조를 보고 '그래디언트가 역전파된다'라고 말합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 조정하는 방법 정리하기\n",
    "\n",
    "로지스틱 회귀의 가중치를 조정하려면, 로지스틱 손실 함수를 가중치에 대해 미분한 식을 가중치에서 빼면 됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "w_i &= w_i - \\frac{\\partial{L}}{\\partial{w_i}} \\\\\n",
    "    &= w_i + (y - a) x_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편향 조정하는 방법 정리하기\n",
    "로지스틱 손실 함수를 편향에 대하여 미분하는 방법도 연쇄 법칙을 적용하면 쉽게 구할 수 있습니다. 앞에서 이미 계산한 것처럼 그 결과를 적용하면 다음을 알 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{z}} \n",
    "&= \\frac{\\partial{L}}{\\partial{a}} \\frac{\\partial{a}}{\\partial{z}} \\\\\n",
    "&= -(y-a)\n",
    "\\end{align}\n",
    "\n",
    "따라서 다음과 같은 식이 성립됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial{L}}{\\partial{b}} \n",
    "&= \\frac{\\partial{L}}{\\partial{z}} \\frac{\\partial{z}}{\\partial{b}} \\\\\n",
    "&= -(y-a) \\frac{\\partial{}}{\\partial{b}}(b + \\sum_{i=1}^{n} w_ix_i)  \\\\\n",
    "&= -(y-a)1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서, $z = b + \\sum_{i=1}^n w_ix_i$ 를 적용하였고, 이 부분을 $b$에 대해 편미분할 경우, $w_ix_i$항들은 모두 $b$에 대해 상수이므로 0가 되고, b는 1이 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편향을 조정하는 것 역시 로지스틱 손실 함수를 편향에 대해 미분한 식을 편향에서 빼면 됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "b &= b - \\frac{\\partial{L}}{\\partial{b}} \\\\\n",
    "  &= b + (y - a) 1\n",
    "\\end{align}\n",
    "\n",
    "이제 가중치와 편향을 업데이트할 수 있는 방법을 모두 알았습니다. 이진 분류를 위한 클래스를 구현하는 것도 어렵지 않을 것입니다. 이제 본격적인 이진 분류를 위하여 분류용 데이터셋을 준비하겠습니다. 이번에 사용할 데이터셋은 위스콘신 유방암 데이터셋입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "\n",
    "1. 케라스 창시자에게 배우는 딥러닝, 프랑소와 숄레, 길벗\n",
    "1. 핸즈온 머신러닝, 오렐리앙 제롱, 한빛미디어\n",
    "1. 딥러닝 입문, 박해선, 이지스 퍼블리싱\n",
    "1. 파이썬으로 배우는 기계학습, 김영섭, K-MOOC\n",
    "1. [로지스틱 회귀 개념 정리](https://eunsukimme.github.io/ml/2019/10/22/Logistic-Regression/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
